{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Kenza Bouqdir - LAB05**"
      ],
      "metadata": {
        "id": "jK7nAK4D5mz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E0_pPJLm4SpT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import bisect\n",
        "from nltk.tree import Tree\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "import requests\n",
        "from io import StringIO\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages if not available\n",
        "try:\n",
        "    import transformers\n",
        "    transformers_available = True\n",
        "except ImportError:\n",
        "    print(\"Installing transformers library...\")\n",
        "    !pip install -q transformers\n",
        "    import transformers\n",
        "    transformers_available = True"
      ],
      "metadata": {
        "id": "rVKNFEwW4X13"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install AllenNLP and necessary dependencies\n",
        "try:\n",
        "    import allennlp\n",
        "    from allennlp.predictors import Predictor\n",
        "    from allennlp.data.tokenizers import SpacyTokenizer\n",
        "    allennlp_available = True\n",
        "except ImportError:\n",
        "    print(\"Installing AllenNLP...\")\n",
        "    !pip install -q allennlp==2.10.1 allennlp-models==2.10.1\n",
        "    try:\n",
        "        import allennlp\n",
        "        from allennlp.predictors import Predictor\n",
        "        from allennlp.data.tokenizers import SpacyTokenizer\n",
        "        allennlp_available = True\n",
        "    except ImportError:\n",
        "        print(\"Failed to install AllenNLP. Using basic coreference resolution.\")\n",
        "        allennlp_available = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vuFgKd14ag3",
        "outputId": "9ba20d75-b225-4233-bcf1-b747b71ee942"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing AllenNLP...\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.2.0 Requires-Python ==3.6\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch<1.13.0,>=1.10.0 (from allennlp) (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch<1.13.0,>=1.10.0\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to install AllenNLP. Using basic coreference resolution.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"SpaCy model not found. Downloading now...\")\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "AlZUFUGE4ep-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GAP dataset URLs - using direct URLs instead of relying on local files\n",
        "GAP_URLS = {\n",
        "    'training': 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv',\n",
        "    'testing': 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv',\n",
        "    'validation': 'https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv'\n",
        "}"
      ],
      "metadata": {
        "id": "0XPh5bgF4jCz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_gap_dataset(url):\n",
        "    \"\"\"\n",
        "    Download GAP dataset from GitHub if local files are unavailable\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check if download succeeded\n",
        "        return pd.read_csv(StringIO(response.text), sep='\\t')\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        return create_sample_dataset()"
      ],
      "metadata": {
        "id": "X41afVSL4lFD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets():\n",
        "    \"\"\"\n",
        "    Load the GAP datasets for coreference resolution tasks.\n",
        "    Returns training, testing, and validation dataframes.\n",
        "    \"\"\"\n",
        "    print(\"Loading datasets...\")\n",
        "    datasets = {}\n",
        "\n",
        "    # Try loading from local files first\n",
        "    for name, url in GAP_URLS.items():\n",
        "        local_path = f'GapData/gap-{name.split(\"_\")[0]}.tsv'\n",
        "        try:\n",
        "            # Try local file first\n",
        "            datasets[name] = pd.read_csv(local_path, sep='\\t')\n",
        "            print(f\"{name.capitalize()} dataset loaded from local file. Shape: {datasets[name].shape}\")\n",
        "        except FileNotFoundError:\n",
        "            # If not found, download from GitHub\n",
        "            print(f\"Downloading {name} dataset from GitHub...\")\n",
        "            datasets[name] = download_gap_dataset(url)\n",
        "            print(f\"{name.capitalize()} dataset shape: {datasets[name].shape}\")\n",
        "\n",
        "    return datasets['training'], datasets['testing'], datasets['validation']\n"
      ],
      "metadata": {
        "id": "-zxMFH1k4oMV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_dataset():\n",
        "    \"\"\"Create a small sample dataset for demonstration when files are not available\"\"\"\n",
        "    print(\"Creating sample dataset for demonstration...\")\n",
        "    data = {\n",
        "        'ID': [f'sample-{i}' for i in range(1, 11)],\n",
        "        'Text': [\n",
        "            'John saw Mike at the store. He was buying groceries.',\n",
        "            'Mary met Susan after school. She had a new book to share.',\n",
        "            'The professor praised the student because he had solved the difficult problem.',\n",
        "            'Sarah and Rebecca went to the park. She brought a frisbee.',\n",
        "            'The lawyer consulted with the client before she presented the case.',\n",
        "            'The doctor told the patient that he needed to rest.',\n",
        "            'When Tom met Jake at the conference, he was very excited.',\n",
        "            'The mother told her daughter that she should study more.',\n",
        "            'After the meeting, the manager asked the assistant if she could prepare the report.',\n",
        "            'The cat chased the mouse until it escaped under the sofa.'\n",
        "        ],\n",
        "        'Pronoun': ['He', 'She', 'he', 'She', 'she', 'he', 'he', 'she', 'she', 'it'],\n",
        "        'Pronoun-offset': [25, 33, 47, 39, 48, 37, 45, 42, 70, 31],\n",
        "        'A': ['John', 'Mary', 'professor', 'Sarah', 'lawyer', 'doctor', 'Tom', 'mother', 'manager', 'cat'],\n",
        "        'A-offset': [0, 0, 4, 0, 4, 4, 5, 4, 19, 4],\n",
        "        'A-coref': [True, True, True, True, True, False, True, False, True, False],\n",
        "        'B': ['Mike', 'Susan', 'student', 'Rebecca', 'client', 'patient', 'Jake', 'daughter', 'assistant', 'mouse'],\n",
        "        'B-offset': [9, 11, 24, 10, 26, 16, 15, 20, 36, 14],\n",
        "        'B-coref': [False, False, False, False, False, True, False, True, False, True],\n",
        "        'URL': [''] * 10\n",
        "    }\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "7DzjPWJq4t6i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_offset_to_token(tokens, offset):\n",
        "    \"\"\"\n",
        "    Maps a character offset to its token index.\n",
        "\n",
        "    Args:\n",
        "        tokens: List of SpaCy tokens\n",
        "        offset: Character offset to map\n",
        "\n",
        "    Returns:\n",
        "        Index of the token containing the offset, or None if not found\n",
        "    \"\"\"\n",
        "    # Create a list of token start positions\n",
        "    starts = [token.idx for token in tokens]\n",
        "\n",
        "    if not starts or offset < starts[0]:\n",
        "        return None\n",
        "\n",
        "    # Use binary search to find the token\n",
        "    position = bisect.bisect_right(starts, offset) - 1\n",
        "\n",
        "    # Verify the offset is within the token's range\n",
        "    if position >= 0 and position < len(tokens):\n",
        "        token = tokens[position]\n",
        "        if token.idx <= offset < token.idx + len(token.text):\n",
        "            return position\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "1d4119Ot4udD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract preceding tokens\n",
        "def extract_preceding(tokens, offset, k):\n",
        "    \"\"\"\n",
        "    Extract tokens that appear before the current position.\n",
        "\n",
        "    Args:\n",
        "        tokens: List of tokens\n",
        "        offset: Current token position\n",
        "        k: Number of tokens to extract\n",
        "\n",
        "    Returns:\n",
        "        List of preceding tokens, padded with None if needed\n",
        "    \"\"\"\n",
        "    start = max(0, offset - k)\n",
        "    preceding = tokens[start:offset]\n",
        "    return [None] * (k - len(preceding)) + preceding"
      ],
      "metadata": {
        "id": "APFMGg0Q4wv2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract following tokens\n",
        "def extract_following(tokens, offset, k):\n",
        "    \"\"\"\n",
        "    Extract tokens that appear after the current position.\n",
        "\n",
        "    Args:\n",
        "        tokens: List of tokens\n",
        "        offset: Current token position\n",
        "        k: Number of tokens to extract\n",
        "\n",
        "    Returns:\n",
        "        List of following tokens, padded with None if needed\n",
        "    \"\"\"\n",
        "    following = tokens[offset:offset + k]\n",
        "    return following + [None] * (k - len(following))\n"
      ],
      "metadata": {
        "id": "87m5kAK34ypu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert SpaCy dependency tree to NLTK Tree\n",
        "def spacy_to_nltk_tree(token):\n",
        "    \"\"\"\n",
        "    Recursively converts a SpaCy dependency tree to NLTK Tree format.\n",
        "\n",
        "    Args:\n",
        "        token: SpaCy token\n",
        "\n",
        "    Returns:\n",
        "        NLTK Tree\n",
        "    \"\"\"\n",
        "    if list(token.children):\n",
        "        return Tree(f\"{token.dep_}:{token.text}\", [spacy_to_nltk_tree(child) for child in token.children])\n",
        "    else:\n",
        "        return token.text"
      ],
      "metadata": {
        "id": "fcdYEVl-41sA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a parse tree visualization\n",
        "def visualize_parse_tree(text):\n",
        "    \"\"\"\n",
        "    Generate and display an NLTK parse tree for a text.\n",
        "\n",
        "    Args:\n",
        "        text: Text to parse\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    print(f\"Parse tree for: '{text}'\")\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        # Find the root\n",
        "        root = None\n",
        "        for token in sent:\n",
        "            if token.dep_ == \"ROOT\":\n",
        "                root = token\n",
        "                break\n",
        "\n",
        "        if root:\n",
        "            tree = spacy_to_nltk_tree(root)\n",
        "            print(tree)\n",
        "        else:\n",
        "            print(\"Could not find root in sentence.\")\n"
      ],
      "metadata": {
        "id": "76OG-y3i40jA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract comprehensive linguistic features\n",
        "def extract_features(text, char_offset):\n",
        "    \"\"\"\n",
        "    Extract linguistic features for a token at the given character offset.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        char_offset: Character offset of the token of interest\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of features\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    token_idx = map_offset_to_token(list(doc), char_offset)\n",
        "\n",
        "    if token_idx is None:\n",
        "        print(f\"Warning: Could not find token at offset {char_offset} in text: '{text}'\")\n",
        "        return None\n",
        "\n",
        "    # Get token and its sentence\n",
        "    token = doc[token_idx]\n",
        "    sentence = None\n",
        "    for sent in doc.sents:\n",
        "        if sent.start <= token_idx < sent.end:\n",
        "            sentence = sent\n",
        "            break\n",
        "\n",
        "    if not sentence:\n",
        "        print(f\"Warning: Could not find sentence containing token at position {token_idx}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate position in sentence\n",
        "    position_in_sentence = token_idx - sentence.start\n",
        "    sentence_tokens = [t.text for t in sentence]\n",
        "\n",
        "    # Extract features\n",
        "    features = {\n",
        "        \"mention\": token.text,\n",
        "        \"head\": token.head.text,\n",
        "        \"head_pos\": token.head.pos_,\n",
        "        \"sentence_first\": sentence[0].text,\n",
        "        \"sentence_last\": sentence[-1].text,\n",
        "        \"preceding_2\": extract_preceding(sentence_tokens, position_in_sentence, 2),\n",
        "        \"preceding_5\": extract_preceding(sentence_tokens, position_in_sentence, 5),\n",
        "        \"following_2\": extract_following(sentence_tokens, position_in_sentence + 1, 2),\n",
        "        \"following_5\": extract_following(sentence_tokens, position_in_sentence + 1, 5),\n",
        "        \"all_tokens\": sentence_tokens,\n",
        "        \"pos\": token.pos_,\n",
        "        \"dep\": token.dep_,\n",
        "        \"is_pronoun\": token.pos_ == \"PRON\",\n",
        "        \"is_entity\": token.ent_type_ != \"\",\n",
        "        \"entity_type\": token.ent_type_ if token.ent_type_ else None\n",
        "    }\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "OGzTWGIq47LM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AllenNLP-based coreference resolution\n",
        "def predict_with_allennlp(text, pronoun_offset, entity_a_offset, entity_b_offset):\n",
        "    \"\"\"\n",
        "    Use AllenNLP for coreference resolution.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        pronoun_offset: Character offset of the pronoun\n",
        "        entity_a_offset: Character offset of entity A\n",
        "        entity_b_offset: Character offset of entity B\n",
        "\n",
        "    Returns:\n",
        "        'A', 'B', or None depending on coreference\n",
        "    \"\"\"\n",
        "    # Load the pretrained model (will download if not present)\n",
        "    try:\n",
        "        predictor = Predictor.from_path(\n",
        "            \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\"\n",
        "        )\n",
        "\n",
        "        # Run the model to get clusters\n",
        "        result = predictor.predict(document=text)\n",
        "        clusters = result.get(\"clusters\", [])\n",
        "\n",
        "        # Process the document with SpaCy for character-to-token mapping\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Find token indices for offsets\n",
        "        pronoun_idx = map_offset_to_token(list(doc), pronoun_offset)\n",
        "        entity_a_idx = map_offset_to_token(list(doc), entity_a_offset)\n",
        "        entity_b_idx = map_offset_to_token(list(doc), entity_b_offset)\n",
        "\n",
        "        if None in (pronoun_idx, entity_a_idx, entity_b_idx):\n",
        "            print(\"Warning: Could not map all offsets to tokens\")\n",
        "            return None\n",
        "\n",
        "        # Find which cluster contains our pronoun\n",
        "        pronoun_cluster = None\n",
        "        for cluster in clusters:\n",
        "            for span in cluster:\n",
        "                if span[0] <= pronoun_idx <= span[1]:\n",
        "                    pronoun_cluster = cluster\n",
        "                    break\n",
        "            if pronoun_cluster:\n",
        "                break\n",
        "\n",
        "        if not pronoun_cluster:\n",
        "            return None\n",
        "\n",
        "        # Check if entity A or B is in the same cluster\n",
        "        a_in_cluster = any(span[0] <= entity_a_idx <= span[1] for span in pronoun_cluster)\n",
        "        b_in_cluster = any(span[0] <= entity_b_idx <= span[1] for span in pronoun_cluster)\n",
        "\n",
        "        if a_in_cluster and not b_in_cluster:\n",
        "            return \"A\"\n",
        "        elif b_in_cluster and not a_in_cluster:\n",
        "            return \"B\"\n",
        "        elif a_in_cluster and b_in_cluster:\n",
        "            # Both in cluster, use distance heuristic\n",
        "            return \"A\" if abs(pronoun_idx - entity_a_idx) < abs(pronoun_idx - entity_b_idx) else \"B\"\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error using AllenNLP: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "VNVjSH-B47l1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature-based coreference resolution as fallback\n",
        "def resolve_with_features(text, pronoun_offset, entity_a_offset, entity_b_offset):\n",
        "    \"\"\"\n",
        "    Rule-based approach using SpaCy features when AllenNLP is unavailable.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        pronoun_offset: Character offset of pronoun\n",
        "        entity_a_offset: Character offset of entity A\n",
        "        entity_b_offset: Character offset of entity B\n",
        "\n",
        "    Returns:\n",
        "        'A', 'B', or None based on rule-based analysis\n",
        "    \"\"\"\n",
        "    # Extract features for each mention\n",
        "    p_features = extract_features(text, pronoun_offset)\n",
        "    a_features = extract_features(text, entity_a_offset)\n",
        "    b_features = extract_features(text, entity_b_offset)\n",
        "\n",
        "    if not all([p_features, a_features, b_features]):\n",
        "        print(\"Warning: Could not extract features for all mentions\")\n",
        "        return None\n",
        "\n",
        "    doc = nlp(text)\n",
        "    p_token = None\n",
        "    a_token = None\n",
        "    b_token = None\n",
        "\n",
        "    for token in doc:\n",
        "        if token.idx == pronoun_offset:\n",
        "            p_token = token\n",
        "        elif token.idx == entity_a_offset:\n",
        "            a_token = token\n",
        "        elif token.idx == entity_b_offset:\n",
        "            b_token = token\n",
        "\n",
        "    # Get token indices\n",
        "    token_list = list(doc)\n",
        "    p_idx = token_list.index(p_token) if p_token in token_list else -1\n",
        "    a_idx = token_list.index(a_token) if a_token in token_list else -1\n",
        "    b_idx = token_list.index(b_token) if b_token in token_list else -1\n",
        "\n",
        "    if -1 in (p_idx, a_idx, b_idx):\n",
        "        print(\"Warning: Could not find all tokens in document\")\n",
        "        return None\n",
        "\n",
        "    # Rule 1: Gender agreement (for gendered pronouns)\n",
        "    pronoun_lower = p_features[\"mention\"].lower()\n",
        "\n",
        "    # Check if both entities are in the same sentence as the pronoun\n",
        "    same_sentence_a = any(sent.start <= p_idx < sent.end and sent.start <= a_idx < sent.end for sent in doc.sents)\n",
        "    same_sentence_b = any(sent.start <= p_idx < sent.end and sent.start <= b_idx < sent.end for sent in doc.sents)\n",
        "\n",
        "    # Rule 2: Syntactic role (subjects are more likely to be antecedents)\n",
        "    a_is_subject = a_token.dep_ in [\"nsubj\", \"nsubjpass\"] if a_token else False\n",
        "    b_is_subject = b_token.dep_ in [\"nsubj\", \"nsubjpass\"] if b_token else False\n",
        "\n",
        "    # Rule 3: Distance (closer mentions are more likely to be antecedents)\n",
        "    a_distance = abs(p_idx - a_idx)\n",
        "    b_distance = abs(p_idx - b_idx)\n",
        "\n",
        "    # Rule 4: Sentence recency (more recent sentences are preferred)\n",
        "    a_sentence_idx = -1\n",
        "    b_sentence_idx = -1\n",
        "    p_sentence_idx = -1\n",
        "\n",
        "    for i, sent in enumerate(doc.sents):\n",
        "        if sent.start <= p_idx < sent.end:\n",
        "            p_sentence_idx = i\n",
        "        if sent.start <= a_idx < sent.end:\n",
        "            a_sentence_idx = i\n",
        "        if sent.start <= b_idx < sent.end:\n",
        "            b_sentence_idx = i\n",
        "\n",
        "    a_sentence_distance = abs(p_sentence_idx - a_sentence_idx)\n",
        "    b_sentence_distance = abs(p_sentence_idx - b_sentence_idx)\n",
        "\n",
        "    # Combine rules with weights\n",
        "    score_a = 0\n",
        "    score_b = 0\n",
        "\n",
        "    # Syntax weight\n",
        "    if a_is_subject:\n",
        "        score_a += 2\n",
        "    if b_is_subject:\n",
        "        score_b += 2\n",
        "\n",
        "    # Same sentence weight\n",
        "    if same_sentence_a:\n",
        "        score_a += 1.5\n",
        "    if same_sentence_b:\n",
        "        score_b += 1.5\n",
        "\n",
        "    # Sentence distance weight (if not in same sentence)\n",
        "    if not same_sentence_a:\n",
        "        score_a -= a_sentence_distance * 0.5\n",
        "    if not same_sentence_b:\n",
        "        score_b -= b_sentence_distance * 0.5\n",
        "\n",
        "    # Token distance weight (only if in same sentence)\n",
        "    if same_sentence_a:\n",
        "        score_a -= a_distance * 0.1\n",
        "    if same_sentence_b:\n",
        "        score_b -= b_distance * 0.1\n",
        "\n",
        "    # Return the entity with the higher score\n",
        "    if score_a > score_b:\n",
        "        return \"A\"\n",
        "    elif score_b > score_a:\n",
        "        return \"B\"\n",
        "    else:\n",
        "        # Tiebreaker: Return closest entity\n",
        "        return \"A\" if a_distance <= b_distance else \"B\"\n"
      ],
      "metadata": {
        "id": "s5GUqngm4-Z5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main coreference resolution function\n",
        "def resolve_coreference(text, pronoun_offset, entity_a_offset, entity_b_offset):\n",
        "    \"\"\"\n",
        "    Resolve coreference using the best available method.\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        pronoun_offset: Character offset of pronoun\n",
        "        entity_a_offset: Character offset of entity A\n",
        "        entity_b_offset: Character offset of entity B\n",
        "\n",
        "    Returns:\n",
        "        'A', 'B', or None depending on which entity the pronoun refers to\n",
        "    \"\"\"\n",
        "    # Try AllenNLP first if available\n",
        "    if allennlp_available:\n",
        "        result = predict_with_allennlp(text, pronoun_offset, entity_a_offset, entity_b_offset)\n",
        "        if result:\n",
        "            return result\n",
        "        print(\"AllenNLP coreference resolution failed, falling back to feature-based approach\")\n",
        "\n",
        "    # Fall back to feature-based approach\n",
        "    return resolve_with_features(text, pronoun_offset, entity_a_offset, entity_b_offset)\n"
      ],
      "metadata": {
        "id": "WQnsCiZr5HMk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate coreference resolution\n",
        "def evaluate_coref_resolution(dataset, max_samples=10):\n",
        "    \"\"\"\n",
        "    Evaluate coreference resolution on the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: DataFrame with coreference annotations\n",
        "        max_samples: Maximum number of samples to evaluate\n",
        "\n",
        "    Returns:\n",
        "        Accuracy and detailed results\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    results = []\n",
        "\n",
        "    for idx, row in dataset.head(max_samples).iterrows():\n",
        "        try:\n",
        "            text = row[\"Text\"]\n",
        "            pronoun_offset = int(row[\"Pronoun-offset\"])\n",
        "            entity_a_offset = int(row[\"A-offset\"])\n",
        "            entity_b_offset = int(row[\"B-offset\"])\n",
        "\n",
        "            # Ground truth\n",
        "            gold = \"A\" if row[\"A-coref\"] else \"B\" if row[\"B-coref\"] else None\n",
        "\n",
        "            # Prediction\n",
        "            pred = resolve_coreference(text, pronoun_offset, entity_a_offset, entity_b_offset)\n",
        "\n",
        "            # Check if correct\n",
        "            is_correct = (pred == gold)\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "            count += 1\n",
        "\n",
        "            # Store result for detailed analysis\n",
        "            results.append({\n",
        "                \"id\": row[\"ID\"],\n",
        "                \"pronoun\": row[\"Pronoun\"],\n",
        "                \"entity_a\": row[\"A\"],\n",
        "                \"entity_b\": row[\"B\"],\n",
        "                \"gold\": gold,\n",
        "                \"pred\": pred,\n",
        "                \"correct\": is_correct\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing example {idx}: {e}\")\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct / count) * 100 if count > 0 else 0\n",
        "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{count})\")\n",
        "\n",
        "    return accuracy, results"
      ],
      "metadata": {
        "id": "02A3Bxid5M1A"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Improved Coreference Resolution Lab\")\n",
        "\n",
        "    # Load datasets\n",
        "    training_data, testing_data, validation_data = load_datasets()\n",
        "\n",
        "    # Show a sample\n",
        "    print(\"\\nSample data:\")\n",
        "    print(training_data.head(3))\n",
        "\n",
        "    # Test feature extraction\n",
        "    print(\"\\nTesting feature extraction:\")\n",
        "    example = \"John saw Mary at the park yesterday. She waved at him.\"\n",
        "    pronoun_offset = example.find(\"She\")\n",
        "    features = extract_features(example, pronoun_offset)\n",
        "\n",
        "    if features:\n",
        "        print(f\"Text: {example}\")\n",
        "        print(f\"Pronoun: {features['mention']}\")\n",
        "        print(f\"Previous 2 tokens: {features['preceding_2']}\")\n",
        "        print(f\"Next 2 tokens: {features['following_2']}\")\n",
        "        print(f\"Full sentence: {' '.join(features['all_tokens'])}\")\n",
        "\n",
        "    # Visualize parse tree\n",
        "    print(\"\\nVisualizing parse tree:\")\n",
        "    sentence = \"John gave Mary a book because he thought she would enjoy it.\"\n",
        "    visualize_parse_tree(sentence)\n",
        "\n",
        "    # Test coreference resolution with a simple example\n",
        "    print(\"\\nTesting coreference resolution:\")\n",
        "    test_example = \"John met Mike at the store. He was buying groceries.\"\n",
        "    test_pronoun_offset = test_example.find(\"He\")\n",
        "    test_john_offset = test_example.find(\"John\")\n",
        "    test_mike_offset = test_example.find(\"Mike\")\n",
        "\n",
        "    test_result = resolve_coreference(test_example, test_pronoun_offset, test_john_offset, test_mike_offset)\n",
        "    print(f\"Example: {test_example}\")\n",
        "    print(f\"Prediction: 'He' refers to {'John' if test_result == 'A' else 'Mike' if test_result == 'B' else 'neither'}\")\n",
        "\n",
        "    # Evaluate on test dataset\n",
        "    print(\"\\nEvaluating on test dataset (first 10 samples)...\")\n",
        "    accuracy, results = evaluate_coref_resolution(testing_data, max_samples=10)\n",
        "\n",
        "    # Print detailed results\n",
        "    print(\"\\nDetailed results:\")\n",
        "    for i, res in enumerate(results):\n",
        "        print(f\"{i+1}. {res['pronoun']} refers to {res['entity_a'] if res['pred'] == 'A' else res['entity_b'] if res['pred'] == 'B' else 'neither'}\")\n",
        "        print(f\"   Gold: {res['entity_a'] if res['gold'] == 'A' else res['entity_b'] if res['gold'] == 'B' else 'neither'}\")\n",
        "        print(f\"   Result: {'✓' if res['correct'] else '✗'}\")\n",
        "\n",
        "    # Report on library availability\n",
        "    print(\"\\nLibrary status:\")\n",
        "    print(f\"- AllenNLP available: {allennlp_available}\")\n",
        "    print(f\"- Transformers available: {transformers_available}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fBTU8yX5NOc",
        "outputId": "d0b95641-5f28-4eb5-94a1-814ef411e1d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved Coreference Resolution Lab\n",
            "Loading datasets...\n",
            "Downloading training dataset from GitHub...\n",
            "Training dataset shape: (2000, 11)\n",
            "Downloading testing dataset from GitHub...\n",
            "Testing dataset shape: (2000, 11)\n",
            "Downloading validation dataset from GitHub...\n",
            "Validation dataset shape: (454, 11)\n",
            "\n",
            "Sample data:\n",
            "       ID                                               Text Pronoun  \\\n",
            "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
            "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
            "2  test-3  Though his emigration from the country has aff...      He   \n",
            "\n",
            "   Pronoun-offset             A  A-offset  A-coref                   B  \\\n",
            "0             383     Bob Suter       352    False              Dehner   \n",
            "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
            "2             312  Ali Aladhadh       256     True              Saddam   \n",
            "\n",
            "   B-offset  B-coref                                           URL  \n",
            "0       366     True    http://en.wikipedia.org/wiki/Jeremy_Dehner  \n",
            "1       390    False  http://en.wikipedia.org/wiki/Norberto_Alonso  \n",
            "2       295    False         http://en.wikipedia.org/wiki/Aladhadh  \n",
            "\n",
            "Testing feature extraction:\n",
            "Text: John saw Mary at the park yesterday. She waved at him.\n",
            "Pronoun: She\n",
            "Previous 2 tokens: [None, None]\n",
            "Next 2 tokens: ['waved', 'at']\n",
            "Full sentence: She waved at him .\n",
            "\n",
            "Visualizing parse tree:\n",
            "Parse tree for: 'John gave Mary a book because he thought she would enjoy it.'\n",
            "(ROOT:gave\n",
            "  John\n",
            "  Mary\n",
            "  (dobj:book a)\n",
            "  (advcl:thought because he (ccomp:enjoy she would it))\n",
            "  .)\n",
            "\n",
            "Testing coreference resolution:\n",
            "Example: John met Mike at the store. He was buying groceries.\n",
            "Prediction: 'He' refers to John\n",
            "\n",
            "Evaluating on test dataset (first 10 samples)...\n",
            "Accuracy: 70.00% (7/10)\n",
            "\n",
            "Detailed results:\n",
            "1. her refers to Pauline\n",
            "   Gold: Cheryl Cassidy\n",
            "   Result: ✗\n",
            "2. His refers to MacKenzie\n",
            "   Gold: MacKenzie\n",
            "   Result: ✓\n",
            "3. his refers to De la Sota\n",
            "   Gold: De la Sota\n",
            "   Result: ✓\n",
            "4. his refers to Henry Rosenthal\n",
            "   Gold: Henry Rosenthal\n",
            "   Result: ✓\n",
            "5. She refers to Rivera\n",
            "   Gold: Rivera\n",
            "   Result: ✓\n",
            "6. She refers to Collins\n",
            "   Gold: Collins\n",
            "   Result: ✓\n",
            "7. his refers to Akiva Eiger\n",
            "   Gold: neither\n",
            "   Result: ✗\n",
            "8. his refers to Robert Christgau\n",
            "   Gold: Robert Christgau\n",
            "   Result: ✓\n",
            "9. her refers to Kelsey\n",
            "   Gold: Kelsey\n",
            "   Result: ✓\n",
            "10. she refers to Kirstine Stewart\n",
            "   Gold: Christina Jennings\n",
            "   Result: ✗\n",
            "\n",
            "Library status:\n",
            "- AllenNLP available: False\n",
            "- Transformers available: True\n"
          ]
        }
      ]
    }
  ]
}