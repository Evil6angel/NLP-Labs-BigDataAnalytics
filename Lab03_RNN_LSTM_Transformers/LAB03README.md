# Lab 03 – Siamese Similarity (RNN vs LSTM) & Transformer Summarization

- Part I – Siamese network (Quora Question Pairs): build two Siamese variants (RNN and LSTM), implement TripletLoss, train (Adam, epochs/batch per spec), compare validation accuracy and training time.

- Part II – Summarization with different attention types: load three transformer models (Multi-Head, Multi-Query, Grouped-Query), summarize three diverse paragraphs, analyze quality/efficiency differences by attention mechanism.

## Tech: Python, PyTorch/TensorFlow (your choice used), Hugging Face Transformers, Google Colab